# LexIntel - AI-Powered Legal Research Platform

> Intelligent document management and RAG-powered case research platform for law firms

![Status](https://img.shields.io/badge/status-MVP-blue)
![Python](https://img.shields.io/badge/python-3.11+-blue)
![FastAPI](https://img.shields.io/badge/fastapi-0.109.0-green)
![License](https://img.shields.io/badge/license-MIT-green)

## ğŸ¯ Overview

**LexIntel** is an AI-powered legal research platform designed for law firms to efficiently manage cases, upload documents, and leverage Retrieval-Augmented Generation (RAG) to conduct intelligent case research. The platform combines full-text search, semantic search using vector embeddings, and streaming AI-powered chat to help lawyers make better decisions faster.

### Key Capabilities
- ğŸ“„ **Document Management**: Upload and organize case documents (PDFs, Word, TXT)
- ğŸ” **Dual Search**: Full-text search + semantic search with vector embeddings
- ğŸ¤– **RAG Chat**: AI-powered streaming chat with document context awareness
- âš¡ **Async Processing**: Background document processing with Celery workers
- ğŸ¢ **Multi-Case Management**: Organize documents by cases with custom tagging
- ğŸ“Š **Local Development**: Complete Docker Compose setup with PostgreSQL, Redis, and Azurite

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        FastAPI Backend (Port 8000)      â”‚
â”‚  - Cases CRUD                           â”‚
â”‚  - Document Upload & Management         â”‚
â”‚  - Search APIs (full-text + semantic)   â”‚
â”‚  - Chat/RAG APIs (TODO)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚           â”‚           â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚PostgreSQLâ”‚  â”‚ Redis â”‚  â”‚ Azurite  â”‚
â”‚ +pgvectorâ”‚  â”‚(Queue)â”‚  â”‚(Storage) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Celery Workers (Background)â”‚
â”‚  - Text Extraction          â”‚
â”‚  - Embedding Generation     â”‚
â”‚  - Document Processing      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Docker & Docker Compose** (recommended for local development)
- **Node.js 20+** (for OpenAI API key generation)
- **Python 3.11+** (if running without Docker)
- **OpenAI API Key** (get from https://platform.openai.com/account/api-keys)

### Installation

#### 1. Clone Repository
```bash
git clone git@github.com-personalwork:Smeet23/Lexintel.git
cd Lexintel
```

#### 2. Set Up Environment Variables
```bash
cp backend/.env.example backend/.env

# Edit backend/.env and add your OpenAI API key
OPENAI_API_KEY=sk-your-key-here
```

#### 3. Start Services with Docker Compose
```bash
docker-compose up -d
```

This starts:
- **PostgreSQL** on port 5432
- **Redis** on port 6379
- **Azurite** (Azure Storage emulator) on ports 10000-10002
- **FastAPI Backend** on port 8000
- **Celery Worker** for async tasks

#### 4. Verify Services
```bash
# Check all containers are running
docker-compose ps

# Check API is responding
curl http://localhost:8000/health
# Expected: {"status": "ok"}
```

---

## ğŸ“š API Documentation

### Base URL
```
http://localhost:8000
```

### Auto-Generated Docs
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Core Endpoints

#### Cases API

**Create Case**
```bash
curl -X POST http://localhost:8000/cases \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Smith v. Jones",
    "case_number": "2024-001",
    "practice_area": "contracts",
    "status": "active",
    "description": "Contract dispute case"
  }'
```

**List Cases**
```bash
curl http://localhost:8000/cases?skip=0&limit=50
```

**Get Case Details**
```bash
curl http://localhost:8000/cases/{case_id}
```

**Update Case**
```bash
curl -X PATCH http://localhost:8000/cases/{case_id} \
  -H "Content-Type: application/json" \
  -d '{"status": "closed"}'
```

**Delete Case**
```bash
curl -X DELETE http://localhost:8000/cases/{case_id}
```

#### Documents API

**Upload Document**
```bash
curl -X POST "http://localhost:8000/documents/upload?case_id={case_id}" \
  -F "file=@/path/to/document.pdf"
```

**Get Document Details**
```bash
curl http://localhost:8000/documents/{document_id}
```

**Delete Document**
```bash
curl -X DELETE http://localhost:8000/documents/{document_id}
```

---

## ğŸ—‚ï¸ Project Structure

```
lex-intel/
â”œâ”€â”€ backend/                          # FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                   # FastAPI app entry point
â”‚   â”‚   â”œâ”€â”€ config.py                 # Configuration & settings
â”‚   â”‚   â”œâ”€â”€ database.py               # Database connection setup
â”‚   â”‚   â”œâ”€â”€ celery_app.py             # Celery configuration
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ models/                   # SQLAlchemy ORM models
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py               # Base model & mixins
â”‚   â”‚   â”‚   â”œâ”€â”€ case.py               # Case model
â”‚   â”‚   â”‚   â””â”€â”€ document.py           # Document & Chat models
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ schemas/                  # Pydantic validation schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ case.py
â”‚   â”‚   â”‚   â”œâ”€â”€ document.py
â”‚   â”‚   â”‚   â””â”€â”€ chat.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ api/                      # API routers
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cases.py              # Cases endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py          # Documents endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py             # Search endpoints (TODO)
â”‚   â”‚   â”‚   â””â”€â”€ chat.py               # Chat/RAG endpoints (TODO)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ services/                 # Business logic services
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ storage.py            # File storage service
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py             # Search service (TODO)
â”‚   â”‚   â”‚   â””â”€â”€ embeddings.py         # Embeddings service (TODO)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ workers/                  # Celery async tasks
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ tasks.py              # Document processing tasks
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/                        # Test suite
â”‚   â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â””â”€â”€ integration/
â”‚   â”‚
â”‚   â”œâ”€â”€ uploads/                      # Document storage (local)
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â”œâ”€â”€ pyproject.toml                # Project configuration
â”‚   â”œâ”€â”€ Dockerfile                    # Docker image for backend
â”‚   â””â”€â”€ .env.example                  # Environment variables template
â”‚
â”œâ”€â”€ docker-compose.yml                # Docker Compose orchestration
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                         # This file
â””â”€â”€ docs/
    â”œâ”€â”€ plans/                        # Implementation plans
    â””â”€â”€ architecture/                 # Architecture diagrams
```

---

## ğŸ› ï¸ Development Workflow

### Running Locally (with Docker)

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f backend

# Stop services
docker-compose down

# Rebuild after dependency changes
docker-compose up --build
```

### Running Without Docker

```bash
# Install Python dependencies
pip install -r backend/requirements.txt

# Create PostgreSQL database
createdb -U postgres lex_intel_dev

# Run migrations
# (Currently handled by SQLAlchemy init_db)

# Start FastAPI backend
cd backend
uvicorn app.main:app --reload --port 8000

# In another terminal, start Celery worker
celery -A app.celery_app worker -l info
```

### Code Quality

```bash
# Format code with Black
black backend/

# Check types with mypy
mypy backend/

# Lint with flake8
flake8 backend/

# Run tests
pytest backend/tests/
```

---

## ğŸ“Š Database Schema

### Core Models

#### Cases
- `id`: Unique identifier
- `name`: Case name (e.g., "Smith v. Jones")
- `case_number`: Case number
- `practice_area`: Legal practice area
- `status`: active/closed/archived
- `description`: Case description
- `created_at`, `updated_at`: Timestamps

#### Documents
- `id`: Unique identifier
- `case_id`: Foreign key to Case
- `title`: Document title
- `filename`: Original filename
- `type`: brief/complaint/discovery/statute/transcript/contract
- `extracted_text`: Full text (after processing)
- `processing_status`: pending/extracted/indexed/failed
- `file_path`: Local storage path
- `created_at`, `updated_at`: Timestamps

#### DocumentChunks
- `id`: Unique identifier
- `document_id`: Foreign key to Document
- `chunk_text`: Text content (4000 char chunks with overlap)
- `chunk_index`: Chunk sequence number
- `embedding`: pgvector embedding (1536 dimensions)
- `search_vector`: PostgreSQL tsvector for full-text search

#### ChatConversations & ChatMessages
- Stores conversation history per case
- Tracks token usage
- Links to source documents

---

## ğŸ”„ Document Processing Pipeline

```
1. Document Upload
   â†“
2. Store in local filesystem
   â†“
3. Queue async processing task
   â†“
4. Extract Text (Celery worker)
   â”œâ”€ PDF â†’ pdf-parse
   â”œâ”€ DOCX â†’ python-pptx
   â””â”€ TXT â†’ direct read
   â†“
5. Split into Chunks (4000 chars, 400 char overlap)
   â†“
6. Generate Embeddings (OpenAI API)
   â†“
7. Store in PostgreSQL with pgvector
   â†“
8. Document ready for search & chat
```

---

## ğŸ” Search Capabilities

### Full-Text Search
- PostgreSQL `tsvector` with `pg_trgm` trigram matching
- Fast keyword search across documents
- Phrase matching support
- Fuzzy matching for typos

### Semantic Search
- OpenAI embeddings (text-embedding-3-small)
- pgvector cosine similarity matching
- Find conceptually similar cases
- Handles synonyms and semantic variations

### Combined Search
- Ranks full-text + semantic results
- Returns hybrid results with confidence scores

---

## ğŸ¤– Async Processing with Celery

### Tasks
- `extract_text_from_document`: Parse PDF/DOCX/TXT files
- `generate_embeddings`: Create OpenAI embeddings
- `process_document_pipeline`: Orchestrate end-to-end pipeline

### Monitoring
```bash
# Check Celery worker logs
docker-compose logs celery-worker

# List active tasks
celery -A app.celery_app inspect active
```

---

## ğŸ” Environment Variables

Required in `backend/.env`:

```bash
# Database
DATABASE_URL=postgresql://lex_user:lex_password@postgres:5432/lex_intel_dev

# Redis
REDIS_URL=redis://redis:6379

# OpenAI (REQUIRED - get from https://platform.openai.com)
OPENAI_API_KEY=sk-your-key-here

# Azurite (Local Azure Storage - pre-configured for Docker)
AZURE_STORAGE_CONNECTION_STRING=...

# App Settings
DEBUG=True
ENVIRONMENT=development
API_HOST=0.0.0.0
API_PORT=8000

# Upload Settings
UPLOAD_DIR=/app/uploads
MAX_UPLOAD_SIZE=104857600  # 100MB
ALLOWED_EXTENSIONS=.pdf,.docx,.txt

# Processing
CHUNK_SIZE=4000
CHUNK_OVERLAP=400
```

---

## ğŸ“ˆ Performance Considerations

### Database
- **Connection Pooling**: Configured via SQLAlchemy
- **Indexes**: On frequently queried fields (case_id, processing_status)
- **Partitioning**: Consider for very large document_chunks table

### Search
- **pgvector**: IVFFlat indexes on embedding column (coming soon)
- **Full-text**: GIN indexes on tsvector column (coming soon)
- **Query Optimization**: Limit chunk retrieval to top-K similar items

### Async Processing
- **Worker Concurrency**: Configured for 4 workers
- **Retry Logic**: Up to 3 retries with exponential backoff
- **Task Timeouts**: 30 min soft, 25 min hard limits

---

## ğŸ—“ï¸ Roadmap

### Current Status: MVP
- âœ… Cases CRUD
- âœ… Document upload & storage
- âœ… Pydantic validation
- âœ… Docker Compose setup
- â³ Text extraction workers
- â³ Embedding generation
- â³ Search APIs
- â³ Chat/RAG APIs

### Coming Soon (Phase 2)
- Authentication with Auth0
- Full-text search API
- Semantic search API
- Document tagging & filtering

### Future (Phase 3+)
- Streaming chat/RAG
- Citation extraction & precedent linking
- Brief/memo generation
- Advanced analytics
- Frontend UI (React)

---

## ğŸ› Troubleshooting

### Docker Issues

**Port already in use**
```bash
# Kill process on port 8000
lsof -ti:8000 | xargs kill -9

# Or change port in docker-compose.yml
```

**Database connection errors**
```bash
# Check PostgreSQL is running
docker-compose logs postgres

# Reset database
docker-compose down -v
docker-compose up -d postgres
```

### API Issues

**500 errors**
```bash
# Check backend logs
docker-compose logs backend

# Check environment variables
docker-compose exec backend env | grep DATABASE
```

**Files not uploading**
```bash
# Check directory permissions
docker-compose exec backend ls -la /app/uploads

# Check max file size (default 100MB)
# Edit backend/.env MAX_UPLOAD_SIZE
```

---

## ğŸ“ Logging

Logs are configured to show:
- `[backend]` - Main API logs
- `[celery]` - Celery worker logs
- `[extract_text]` - Text extraction task logs
- `[storage]` - File storage operations

Example log format:
```
[backend] LexIntel backend starting...
[celery] Starting task: extract_text_from_document (ID: abc123)
[storage] Saved file: /app/uploads/doc-id/document.pdf
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest backend/tests/

# Run specific test
pytest backend/tests/unit/test_cases.py::test_create_case

# Run with coverage
pytest backend/tests/ --cov=app --cov-report=html
```

---

## ğŸ¤ Contributing

1. Create a feature branch: `git checkout -b feature/your-feature`
2. Make changes and test: `pytest backend/tests/`
3. Commit with clear messages: `git commit -m "feat: add feature"`
4. Push to branch: `git push origin feature/your-feature`
5. Create Pull Request on GitHub

### Code Style
- Python: Follow PEP 8 (enforced by Black)
- Type hints: Always use type annotations
- Docstrings: Use Google-style docstrings
- Tests: Write tests for new features

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ“§ Contact & Support

For questions or issues:
- ğŸ“§ Email: smeetagrawal23@gmail.com
- ğŸ™ GitHub: https://github.com/Smeet23/Lexintel
- ğŸ“‹ Issues: Create an issue on GitHub

---

## ğŸ™ Acknowledgments

Built with:
- [FastAPI](https://fastapi.tiangolo.com/) - Modern Python web framework
- [SQLAlchemy](https://www.sqlalchemy.org/) - SQL toolkit and ORM
- [Celery](https://docs.celeryproject.org/) - Distributed task queue
- [pgvector](https://github.com/pgvector/pgvector) - Vector search in PostgreSQL
- [OpenAI](https://openai.com/) - LLM & embedding models
- [Docker](https://www.docker.com/) - Containerization

---

## ğŸ“Š Stats

- **Backend**: ~1,000 lines of Python code
- **Models**: 8 core SQLAlchemy models
- **APIs**: 7 REST endpoints (cases + documents)
- **Services**: 2 services (storage, embeddings)
- **Workers**: 3 Celery tasks
- **Tests**: Unit + Integration tests (coming)

---

**Version**: 0.1.0 (MVP)
**Last Updated**: January 3, 2026
**Status**: ğŸš€ In Development

# LexIntel Worker Architecture Refactoring Design

> Refactor from monolithic backend workers to scalable microservice architecture with real-time progress tracking

**Status**: Design Complete (All 7 Sections Approved)
**Date**: January 3, 2026
**Target**: Phase 3 Implementation

---

## Executive Summary

**Problem**: Current `backend/app/workers/tasks.py` is tightly coupled with API, monolithic, lacks real-time progress tracking, uses complex async/sync patterns.

**Solution**: Refactor to microservice architecture with:
- âœ… Separate `apps/workers/` application (independent microservice)
- âœ… Shared `packages/shared/` for models, schemas, utilities
- âœ… Real-time progress via Redis Pub/Sub + Server-Sent Events (SSE)
- âœ… Pure async/await patterns throughout
- âœ… Type-safe job definitions
- âœ… Graceful shutdown handlers
- âœ… Comprehensive error handling

**Benefits**:
- ðŸ“¦ Independent horizontal scaling (workers â‰  API)
- ðŸš€ Real-time feedback (25ms latency vs 2000ms polling)
- ðŸ”§ Easier to maintain and test (separation of concerns)
- ðŸ“Š Better observability (progress events, structured logging)
- ðŸ›¡ï¸ Production-ready error handling and resilience

---

## Section 1: Directory Structure & File Organization

### New Monorepo Structure

```
lex-intel/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ backend/                    # FastAPI web + API (was backend/)
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ api/               # API routes (cases, documents, search, chat)
â”‚   â”‚   â”‚   â”œâ”€â”€ models/            # Database models (imported from shared)
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas/           # Pydantic schemas (API request/response)
â”‚   â”‚   â”‚   â”œâ”€â”€ services/          # Business logic (storage, search, RAG)
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â””â”€â”€ main.py            # FastAPI app initialization
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚
â”‚   â””â”€â”€ workers/                    # Celery workers (NEW - independent app)
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ __main__.py         # Worker service entry point
â”‚       â”‚   â”œâ”€â”€ celery_app.py       # Celery configuration
â”‚       â”‚   â”œâ”€â”€ config.py           # Worker-specific settings
â”‚       â”‚   â”œâ”€â”€ lib/
â”‚       â”‚   â”‚   â”œâ”€â”€ redis.py        # Redis connection + graceful shutdown
â”‚       â”‚   â”‚   â”œâ”€â”€ progress.py     # Progress tracking via Pub/Sub
â”‚       â”‚   â”‚   â”œâ”€â”€ logging.py      # Structured logging
â”‚       â”‚   â”‚   â””â”€â”€ errors.py       # Worker-specific error handling
â”‚       â”‚   â””â”€â”€ workers/            # Separate worker files by domain
â”‚       â”‚       â”œâ”€â”€ __init__.py
â”‚       â”‚       â”œâ”€â”€ document_extraction.py   # Text extraction
â”‚       â”‚       â”œâ”€â”€ embeddings.py           # Embedding generation
â”‚       â”‚       â””â”€â”€ pipeline.py             # Orchestration
â”‚       â”œâ”€â”€ tests/
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â”œâ”€â”€ pyproject.toml
â”‚       â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ packages/
â”‚   â””â”€â”€ shared/                     # Shared Python package (NEW)
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â””â”€â”€ shared/
â”‚       â”‚       â”œâ”€â”€ __init__.py
â”‚       â”‚       â”œâ”€â”€ models/         # SQLAlchemy models (Document, DocumentChunk, etc)
â”‚       â”‚       â”œâ”€â”€ schemas/
â”‚       â”‚       â”‚   â”œâ”€â”€ jobs.py     # Job type definitions
â”‚       â”‚       â”‚   â””â”€â”€ responses.py
â”‚       â”‚       â”œâ”€â”€ database.py     # Shared async_session
â”‚       â”‚       â”œâ”€â”€ utils/
â”‚       â”‚       â”‚   â”œâ”€â”€ logging.py
â”‚       â”‚       â”‚   â”œâ”€â”€ errors.py
â”‚       â”‚       â”‚   â””â”€â”€ validation.py
â”‚       â”‚       â””â”€â”€ constants.py    # Shared constants
â”‚       â”œâ”€â”€ tests/
â”‚       â”œâ”€â”€ pyproject.toml
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docker-compose.yml              # Services: postgres, redis, backend, workers
â”œâ”€â”€ pyproject.toml                  # Root monorepo config
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md             # Architecture decision document
â”‚   â”œâ”€â”€ WORKERS.md                  # Updated worker docs
â”‚   â””â”€â”€ ...existing docs
â””â”€â”€ .gitignore, etc
```

### Key Changes
- `backend/` â†’ `apps/backend/` (FastAPI web only)
- `backend/app/workers/` â†’ `apps/workers/src/workers/` (separate app)
- New `packages/shared/` with models, schemas, utilities
- Clear separation: API imports from shared, Workers import from shared

---

## Section 2: Worker Architecture & Task Organization

### Problem Solved
Current: Single `tasks.py` with mixed concerns
New: Separate files by domain, with shared infrastructure

### Structure

```
apps/workers/src/workers/

1. document_extraction.py
   - extract_text_from_document()      # Main task
   - _extract_and_chunk_document()     # Async helper
   - Handles: File extraction, text cleaning, chunking, progress tracking

2. embeddings.py
   - generate_embeddings()             # Main task (Phase 4)
   - _generate_and_store_embeddings()  # Async helper
   - Handles: Batch embedding generation, pgvector storage

3. pipeline.py
   - process_document_pipeline()       # Orchestration
   - Handles: Queuing downstream tasks (extract â†’ embeddings â†’ search)
```

### Worker Lifecycle

```
API endpoint â†’ Queue job â†’ Celery broker (Redis)
                              â†“
                        Worker picks up
                              â†“
                        Executes task code
                              â†“
                        Reports progress via Redis Pub/Sub
                              â†“
                        Updates database status
                              â†“
                        Queues next task (if needed)
```

### Task Pattern

Each task file contains:
- Main `@shared_task` decorated function
- Async helper with core logic
- Error handling (permanent vs retryable)
- Logging at each step
- Progress publishing

---

## Section 3: Shared Package - Job Types & Models

### Purpose
Single source of truth for data structures shared by backend (API) and workers (Celery)

### Content

**Database Models** (imported by both):
```python
from shared.models import Document, DocumentChunk, Case, ChatConversation
```

**Job Type Definitions** (type-safe payloads):
```python
from shared.schemas.jobs import DocumentExtractionJob, EmbeddingGenerationJob

class DocumentExtractionJob(BaseModel):
    document_id: str
    case_id: str
    source: str = "upload"

class EmbeddingGenerationJob(BaseModel):
    document_id: str
    chunk_ids: Optional[list[str]] = None
```

**Utilities** (used by both):
```python
from shared.utils.errors import PermanentError, RetryableError
from shared.utils.logging import setup_logging
from shared.database import async_session, init_db
```

### Benefits
- âœ… No duplication between API and workers
- âœ… Type-safe job payloads (Pydantic validation)
- âœ… Single place to evolve schemas
- âœ… Shared database models (same ORM objects)
- âœ… Easy to test (shared fixtures)

---

## Section 4: Real-Time Progress Tracking with SSE + Redis Pub/Sub

### Architecture

```
Celery Worker â†’ publishes to Redis â†’ FastAPI subscribes â†’ SSE streams â†’ Browser EventSource
```

### Why SSE?

**vs Polling**:
- Latency: 25ms (SSE) vs 2000ms (polling) = 80x faster
- Scalability: 10,000+ docs vs 400 docs before issues
- Efficiency: No unnecessary requests

**vs WebSockets**:
- Simplicity: Standard HTTP vs protocol upgrade
- Memory: 2KB per connection vs 15KB per WebSocket
- Scalability: Sufficient for progress tracking (don't need bidirectional)
- Mobile: Better battery/network handling

### Implementation

**Worker publishes progress**:
```python
@shared_task
async def extract_text_from_document(self, job_payload: dict):
    publisher = ProgressPublisher(redis_client)

    await publisher.publish_progress(
        document_id, 0, "extracting", "Starting..."
    )
    # ... do work ...
    await publisher.publish_progress(
        document_id, 100, "completed", "Done!"
    )
```

**FastAPI streams via SSE**:
```python
@router.get("/documents/{document_id}/progress")
async def stream_document_progress(document_id: str):
    async def event_generator():
        pubsub = redis_client.pubsub()
        await pubsub.subscribe(f"progress:{document_id}")
        async for message in pubsub.listen():
            yield f"data: {message['data']}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")
```

**Browser subscribes**:
```javascript
const eventSource = new EventSource(`/api/documents/${docId}/progress`);
eventSource.onmessage = (event) => {
    const { progress, step, message } = JSON.parse(event.data);
    updateUI(progress, step, message);
};
```

### Performance
- Latency: 25ms typical (sub-second user feedback)
- Scalability: 10,000+ concurrent documents before Redis becomes bottleneck
- Memory: ~200 bytes per event
- Implementation: 4 hours total work

---

## Section 5: Error Handling & Resilience

### Error Classification

**Permanent Errors** (don't retry):
- File not found
- Invalid data format
- Document not in database
- Action: Fail immediately, update status to FAILED

**Retryable Errors** (retry with backoff):
- Database connection timeout
- Network temporary failure
- Redis connection issue
- Action: Retry 3 times with exponential backoff (60s, 120s, 240s)

### Implementation Pattern

```python
@shared_task(base=CallbackTask, bind=True, max_retries=3)
async def extract_text_from_document(self, job_payload: dict):
    try:
        # Main logic
        pass

    except PermanentError as e:
        # Don't retry
        await update_document_status(doc_id, ProcessingStatus.FAILED, str(e))
        raise  # Fail task

    except RetryableError as e:
        # Retry with backoff
        await update_document_status(doc_id, ProcessingStatus.PENDING)
        raise self.retry(exc=e, countdown=60 * (2 ** self.request.retries))

    except Exception as e:
        # Unknown - treat as retryable
        raise self.retry(exc=e, countdown=60)
```

### Celery Configuration

```python
celery_app.conf.update(
    task_acks_late=True,  # Acknowledge after completion
    worker_prefetch_multiplier=1,  # One task at a time
    task_reject_on_worker_lost=True,  # Reject if worker dies
    task_soft_time_limit=25 * 60,  # 25 min soft timeout
    task_time_limit=30 * 60,  # 30 min hard timeout
    task_track_started=True,  # Track STARTED state
)
```

### Benefits
- âœ… Safe failure (DB updates even if publishing fails)
- âœ… Exponential backoff prevents thundering herd
- âœ… Task loss prevention (acks_late)
- âœ… Worker crash handling (reject_on_worker_lost)
- âœ… Timeout protection (soft + hard limits)

---

## Section 6: Database & Pure Async Patterns

### Problem Solved
Current: `asyncio.get_event_loop().run_until_complete()` (mixing boundaries)
New: Pure async/await throughout (Celery 5.3+ supports async tasks)

### Shared Database Layer

```python
# packages/shared/src/shared/database.py
# Used by both backend AND workers

from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession

engine = create_async_engine(DATABASE_URL, pool_size=20, max_overflow=10)
async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

# Both backend and workers import this
from shared.database import async_session
```

### Worker Implementation

```python
@shared_task(base=CallbackTask, bind=True, max_retries=3)
async def extract_text_from_document(self, job_payload: dict):
    # Pure async - no event loop hacks!
    job = DocumentExtractionJob(**job_payload)

    async with async_session() as session:
        # Query
        document = await session.get(Document, job.document_id)

        # Process
        text = await extract_file(document.file_path)
        chunks = await create_text_chunks(job.document_id, text, session)

        # Update & commit
        document.processing_status = ProcessingStatus.EXTRACTED
        await session.commit()

    return {"status": "success", "chunks": len(chunks)}
```

### Graceful Shutdown

```python
# apps/workers/src/__main__.py
def shutdown_handler(signum, frame):
    """Handle SIGTERM/SIGINT"""
    logger.info("Gracefully stopping worker...")
    celery_app.control.shutdown()
    asyncio.run(close_db())
    sys.exit(0)

signal.signal(signal.SIGTERM, shutdown_handler)
signal.signal(signal.SIGINT, shutdown_handler)
```

### Benefits
- âœ… Clean code (no event loop hacks)
- âœ… Shared models (both use same ORM objects)
- âœ… Connection pooling (efficient DB access)
- âœ… Graceful deployment (clean shutdown)
- âœ… Type safety (SQLAlchemy async typing)

---

## Section 7: Testing Strategy

### Test Structure

```
apps/workers/tests/
â”œâ”€â”€ conftest.py              # Shared fixtures
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_document_extraction.py
â”‚   â”œâ”€â”€ test_embeddings.py
â”‚   â””â”€â”€ test_pipeline.py
â””â”€â”€ integration/
    â”œâ”€â”€ test_extraction_workflow.py
    â””â”€â”€ test_progress_tracking.py
```

### Unit Tests (mocked dependencies)
- Success path (extraction completes)
- Permanent errors (file not found, no retry)
- Transient errors (DB timeout, with retry)
- Progress publishing (events sent)
- Progress on error (failure state shown)

### Integration Tests (real database)
- Full workflow (file â†’ DB â†’ chunks)
- Database state updates (status changes)
- Chunk creation and indexing
- Error recovery (retry handling)
- Concurrent document processing

### Fixtures
```python
@pytest.fixture
async def async_db_session():
    """Provide test database"""
    await init_db()
    yield async_session
    await close_db()

@pytest.fixture
def mock_redis():
    """Mock Redis for progress"""
    return AsyncMock()
```

### Coverage Goals
- Target: >85% code coverage
- All error paths tested
- Progress tracking verified
- Concurrency scenarios included

---

## Implementation Checklist

### Before Implementation
- [ ] Review design with team
- [ ] Create git worktree for isolated work
- [ ] Set up shared package structure
- [ ] Create packages/shared/pyproject.toml

### Migration Phase (Day 1-2)
- [ ] Move backend/ â†’ apps/backend/
- [ ] Extract shared code â†’ packages/shared/
- [ ] Update imports in apps/backend/
- [ ] Update Docker Compose volumes

### Worker Refactoring (Day 2-3)
- [ ] Create apps/workers/ directory
- [ ] Move tasks.py â†’ separate worker files
- [ ] Implement graceful shutdown
- [ ] Add progress tracking (SSE)
- [ ] Improve error handling

### Testing (Day 3-4)
- [ ] Write unit tests
- [ ] Write integration tests
- [ ] Test error scenarios
- [ ] Test progress tracking
- [ ] End-to-end API test

### Documentation (Day 4)
- [ ] Update ARCHITECTURE.md
- [ ] Update WORKERS.md
- [ ] Document configuration
- [ ] Update claude.md

### Deployment (Day 5)
- [ ] Test in Docker Compose
- [ ] Verify both services start
- [ ] Test worker independence
- [ ] Test scaling (multiple workers)

---

## Success Criteria

### Architectural
- [ ] Workers in separate `apps/workers/` directory
- [ ] Shared code in `packages/shared/`
- [ ] Backend and workers import from shared
- [ ] No circular dependencies
- [ ] Can deploy workers independently

### Functional
- [ ] Text extraction works as before
- [ ] Progress tracking in real-time (SSE)
- [ ] Error handling (permanent vs retryable)
- [ ] Graceful shutdown handling
- [ ] All tests pass (>85% coverage)

### Operational
- [ ] Docker Compose runs both services
- [ ] Workers scale horizontally (multiple instances)
- [ ] No breaking API changes
- [ ] Database migrations not needed
- [ ] Monitoring/logging improved

---

## Timeline

**Estimated**: 4-5 days full-time work

- Day 1: File structure migration
- Day 2: Worker refactoring + SSE setup
- Day 3: Error handling + progress tracking
- Day 4: Comprehensive testing
- Day 5: Documentation + deployment verification

**Parallel with**: Continue Phase 4 planning (embeddings)

---

## Migration Path

### Phase 1: Structure (No functional changes)
- Move files to new structure
- Update imports
- Docker Compose still works
- All functionality same

### Phase 2: Features (Enhanced functionality)
- Add real-time progress
- Improve error handling
- Add graceful shutdown
- Same external API

### Phase 3: Scalability (Optional future)
- Independent worker deployment
- Horizontal scaling
- Multi-region support
- Enhanced monitoring

---

## Open Questions / Decisions Made

âœ… **Architecture**: Monorepo with apps/ and packages/ (decided)
âœ… **Real-time**: Redis Pub/Sub + SSE (decided)
âœ… **Database**: Shared async_session (decided)
âœ… **Async**: Pure async/await throughout (decided)
âœ… **Testing**: Unit + Integration with >85% coverage (decided)

---

## Next Steps

1. **Review Design**: Get stakeholder approval
2. **Create Worktree**: `git worktree add refactoring origin/main`
3. **Write Implementation Plan**: Detailed 50+ task breakdown
4. **Execute**: Subagent-driven development with code review checkpoints
5. **Deploy**: Test in Docker Compose, verify all services work

---

**Design Status**: âœ… **COMPLETE & APPROVED**
**Ready for**: Implementation Planning
**Target Start**: Today
**Target Completion**: End of this week

